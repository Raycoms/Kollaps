#topology

apiVersion: v1
kind: ConfigMap
metadata:
  name: topology
data:
  topology.xml: "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<experiment boot=\"need:1.1\">\n\t<services>\n\t\t<service
    name=\"dashboard\" image=\"warpenguin.no-ip.org/dashboard:1.0\" supervisor=\"true\"
    port=\"8088\"/>\n\t\t<service name=\"logger\"  image=\"warpenguin.no-ip.org/logger:1.0\"
    supervisor=\"true\" />\n\t\t<service name=\"client1\" image=\"warpenguin.no-ip.org/alpineclient:1.0\"
    command=\"['server', '0', '0']\"/>\n\t\t<service name=\"client2\" image=\"warpenguin.no-ip.org/alpineclient:1.0\"
    command=\"['server', '0', '0']\"/>\n\t\t<service name=\"client3\" image=\"warpenguin.no-ip.org/alpineclient:1.0\"
    command=\"['server', '0', '0']\"/>\n\t\t<service name=\"server\" image=\"warpenguin.no-ip.org/alpineserver:1.0\"
    share=\"false\"/>\n\t</services>\n\t<bridges>\n                <bridge name=\"s1\"/>\n
    \               <bridge name=\"s2\"/>\n\t</bridges>\n  <links>\n                <link
    origin=\"client1\" dest=\"s1\" latency=\"10\" upload=\"100Mbps\" download=\"100Mbps\"
    network=\"test_overlay\"/>\n                <link origin=\"client2\" dest=\"s1\"
    latency=\"5\" upload=\"100Mbps\" download=\"100Mbps\" network=\"test_overlay\"/>\n
    \               <link origin=\"client3\" dest=\"s1\" latency=\"5\" upload=\"10Mbps\"
    download=\"10Mbps\" network=\"test_overlay\"/>\n                <link origin=\"s1\"
    dest=\"s2\" latency=\"10\" upload=\"50Mbps\" download=\"50Mbps\" network=\"test_overlay\"/>\n
    \               <link origin=\"s2\" dest=\"server\" latency=\"5\" upload=\"100Mbps\"
    download=\"100Mbps\" network=\"test_overlay\"/>\n\t</links>\n\t<dynamic>\n\t\t<schedule
    name=\"client1\" time=\"0.0\" action=\"join\"/>\n\t\t<schedule name=\"client2\"
    time=\"0.0\" action=\"join\"/>\n\t\t<schedule name=\"client3\" time=\"0.0\" action=\"join\"/>\n\t\t<schedule
    name=\"server\"  time=\"0.0\" action=\"join\" amount=\"3\"/>\n\n\t\t<schedule
    name=\"client1\" time=\"10.0\" action=\"crash\"/>\n\t\t<schedule name=\"client2\"
    time=\"20.0\" action=\"crash\"/>\n\t\t<schedule name=\"client3\" time=\"30.0\"
    action=\"crash\"/>\n\t\t<schedule name=\"server\"  time=\"40.0\" action=\"leave\"
    amount=\"3\"/>\n\t</dynamic>\n</experiment>\n"

---
# roles

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: NEED
  name: need-listpods
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: listpods
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: listpods
subjects:
- kind: ServiceAccount
  name: need-listpods
  namespace: default
roleRef:
  kind: ClusterRole
  name: listpods
  apiGroup: rbac.authorization.k8s.io

---
#bootstrapper

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: bootstrapper
  labels:
    bootb60491a1-9da5-4f90-a046-3891a6cf504d: "true"
spec:
  template:
    metadata:
      labels:
        app: NEED
        bootb60491a1-9da5-4f90-a046-3891a6cf504d: 'true'
    spec:
      containers:
      - name: bootstrapper
        image: need:1.1
        env:
        - name: NEED_UUID
          value: b60491a1-9da5-4f90-a046-3891a6cf504d
        args:
        - -g
        - b60491a1-9da5-4f90-a046-3891a6cf504d
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "SYS_ADMIN"]
        volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock
          subPath: docker.sock
        - name: topology
          mountPath: /topology.xml
          subPath: topology.xml
      serviceAccountName: need-listpods
      hostPID: true
      volumes:
      - name: docker-socket
        hostPath:
          path: /run
      - name: topology
        configMap:
          name: topology
          defaultMode: 440

---
# dashboard


apiVersion: v1
kind: Pod
metadata:
  name: dashboard-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  containers:
  - name: dashboard-b60491a1-9da5-4f90-a046-3891a6cf504d
    image: warpenguin.no-ip.org/dashboard:1.0
    env:
      - name: NEED_UUID
        value: b60491a1-9da5-4f90-a046-3891a6cf504d
    ports:
    - containerPort: 8088
    volumeMounts:
    - name: topology
      mountPath: /topology.xml
      subPath: topology.xml
  serviceAccountName: need-listpods
  volumes:
  - name: topology
    configMap:
      name: topology
      defaultMode: 440

---
# logger

apiVersion: v1
kind: Pod
metadata:
  name: logger-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  containers:
    - name: logger-b60491a1-9da5-4f90-a046-3891a6cf504d
      image: warpenguin.no-ip.org/logger:1.0
      env:
        - name: NEED_UUID
          value: b60491a1-9da5-4f90-a046-3891a6cf504d
      volumeMounts:
        - name: topology
          mountPath: /topology.xml
          subPath: topology.xml
  volumes:
  - name: topology
    configMap:
      name: topology
      defaultMode: 440

---
# client1

# We have to create a headless service because pods don't get their own DNS record
# https://github.com/kubernetes/kubernetes/issues/49270
# There is also no DNS round robin as in Docker Swarm
# https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address
apiVersion: v1
kind: Service
metadata:
  name: client1-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  clusterIP: None
  selector:
    app: NEED
---
apiVersion: v1
kind: Pod
metadata:
  name: client1-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
    b60491a1-9da5-4f90-a046-3891a6cf504d: 'true'
spec:
  containers:
  - name: client1-b60491a1-9da5-4f90-a046-3891a6cf504d
    image: warpenguin.no-ip.org/alpineclient:1.0
    env:
      - name: NEED_UUID
        value: b60491a1-9da5-4f90-a046-3891a6cf504d
    command: ["/bin/sh", "-c", "mkfifo /tmp/NEED_hang; exec /bin/sh <> /tmp/NEED_hang #"]
    args: ['server', '0', '0']

---
#client2

apiVersion: v1
kind: Service
metadata:
  name: client2-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  clusterIP: None
  selector:
    app: NEED
---
apiVersion: v1
kind: Pod
metadata:
  name: client2-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
    b60491a1-9da5-4f90-a046-3891a6cf504d: 'true'
spec:
  containers:
  - name: client2-b60491a1-9da5-4f90-a046-3891a6cf504d
    image: warpenguin.no-ip.org/alpineclient:1.0
    env:
      - name: NEED_UUID
        value: b60491a1-9da5-4f90-a046-3891a6cf504d
    command: ["/bin/sh", "-c", "mkfifo /tmp/NEED_hang; exec /bin/sh <> /tmp/NEED_hang #"]
    args: ['server', '0', '0']

---
#client3

apiVersion: v1
kind: Service
metadata:
  name: client3-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  clusterIP: None
  selector:
    app: NEED
---
apiVersion: v1
kind: Pod
metadata:
  name: client3-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
    b60491a1-9da5-4f90-a046-3891a6cf504d: 'true'
spec:
  containers:
  - name: client3-b60491a1-9da5-4f90-a046-3891a6cf504d
    image: warpenguin.no-ip.org/alpineclient:1.0
    env:
      - name: NEED_UUID
        value: b60491a1-9da5-4f90-a046-3891a6cf504d
    command: ["/bin/sh", "-c", "mkfifo /tmp/NEED_hang; exec /bin/sh <> /tmp/NEED_hang #"]
    args: ['server', '0', '0']

---
#server

apiVersion: v1
kind: Service
metadata:
  name: server-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  clusterIP: None
  selector:
    app: NEED
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: server-b60491a1-9da5-4f90-a046-3891a6cf504d
  labels:
    app: NEED
spec:
  selector:
    matchLabels:
      app: NEED
  replicas: 3
  template:
    metadata:
      labels:
        app: NEED
        b60491a1-9da5-4f90-a046-3891a6cf504d: 'true'
    spec:
      containers:
      - name: server-b60491a1-9da5-4f90-a046-3891a6cf504d
        image: warpenguin.no-ip.org/alpineserver:1.0
        env:
        - name: NEED_UUID
          value: b60491a1-9da5-4f90-a046-3891a6cf504d
        command: ["/bin/sh", "-c", "mkfifo /tmp/NEED_hang; exec /bin/sh <> /tmp/NEED_hang #"]
